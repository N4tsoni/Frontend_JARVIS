export interface Article {
  id: string
  categoryId: string
  title: string
  subtitle: string
  icon: string
  content: ArticleSection[]
  relatedArticles?: string[]
}

export interface ArticleSection {
  type: 'paragraph' | 'heading' | 'list' | 'code' | 'highlight'
  content: string
  items?: string[]
  language?: string
}

export const articles: Article[] = [
  // KG Builder Articles
  {
    id: 'pipeline-v3',
    categoryId: 'kg-builder',
    title: 'Pipeline Agentique V3',
    subtitle: 'Pipeline de traitement en 9 √©tapes pour la construction de Knowledge Graphs',
    icon: '‚öôÔ∏è',
    content: [
      {
        type: 'paragraph',
        content: 'La Pipeline V3 repr√©sente une √©volution majeure dans notre approche de construction de Knowledge Graphs. Elle orchestre 9 √©tapes distinctes pour transformer des documents bruts en graphes de connaissances structur√©s.'
      },
      {
        type: 'heading',
        content: 'Les 9 √âtapes de la Pipeline'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Parsing - Extraction du contenu brut depuis diff√©rents formats (PDF, CSV, JSON, etc.)',
          'Chunking - D√©coupage intelligent en pr√©servant la structure hi√©rarchique',
          'Embedding - G√©n√©ration de vecteurs s√©mantiques pour chaque chunk',
          'NER - Reconnaissance des entit√©s nomm√©es (personnes, organisations, lieux...)',
          'Extraction - Extraction des relations entre entit√©s via LLM',
          'Transformation - Normalisation et structuration des donn√©es extraites',
          'Enrichment - Enrichissement avec des sources externes',
          'Validation - V√©rification de la coh√©rence et qualit√© des donn√©es',
          'Storage - Persistance dans Neo4j avec indexation optimis√©e'
        ]
      },
      {
        type: 'heading',
        content: 'Architecture LangGraph'
      },
      {
        type: 'paragraph',
        content: 'La pipeline utilise LangGraph pour orchestrer les diff√©rentes √©tapes de mani√®re flexible et r√©siliente. Chaque √©tape est un n≈ìud du graphe qui peut √™tre ex√©cut√© de mani√®re conditionnelle selon le contexte.'
      },
      {
        type: 'highlight',
        content: 'La Pipeline V3 peut traiter des documents de plus de 100 pages en extrayant automatiquement des centaines d\'entit√©s et relations.'
      }
    ],
    relatedArticles: ['multi-pass', 'hierarchical-chunking', 'graph-aware']
  },
  {
    id: 'multi-pass',
    categoryId: 'kg-builder',
    title: 'Multi-Pass Extraction',
    subtitle: 'Extraction en plusieurs passes pour une couverture maximale',
    icon: 'üîÑ',
    content: [
      {
        type: 'paragraph',
        content: 'L\'extraction Multi-Pass est une technique avanc√©e qui effectue plusieurs passes sur le contenu pour capturer progressivement toutes les entit√©s et relations, m√™me les plus subtiles.'
      },
      {
        type: 'heading',
        content: 'Pourquoi Multi-Pass ?'
      },
      {
        type: 'paragraph',
        content: 'Une seule passe d\'extraction peut manquer des relations implicites ou des entit√©s mentionn√©es indirectement. Le Multi-Pass r√©sout ce probl√®me en effectuant plusieurs analyses avec des focales diff√©rentes.'
      },
      {
        type: 'heading',
        content: 'Les Passes d\'Extraction'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Passe 1 - Extraction des entit√©s principales et relations explicites',
          'Passe 2 - Extraction des entit√©s secondaires et relations implicites',
          'Passe 3 - R√©solution des cor√©f√©rences et consolidation',
          'Passe 4 (optionnelle) - Enrichissement contextuel'
        ]
      },
      {
        type: 'highlight',
        content: 'Le Multi-Pass augmente le recall de l\'extraction de 40% en moyenne par rapport √† une extraction simple.'
      }
    ],
    relatedArticles: ['pipeline-v3', 'graph-aware']
  },
  {
    id: 'graph-aware',
    categoryId: 'kg-builder',
    title: 'Graph-Aware Extraction',
    subtitle: 'Extraction contextuelle bas√©e sur le graphe existant',
    icon: 'üï∏Ô∏è',
    content: [
      {
        type: 'paragraph',
        content: 'L\'extraction Graph-Aware utilise le Knowledge Graph existant comme contexte pour guider l\'extraction de nouvelles informations. Cela permet d\'√©viter les doublons et d\'enrichir les connexions existantes.'
      },
      {
        type: 'heading',
        content: 'Fonctionnement'
      },
      {
        type: 'list',
        content: '',
        items: [
          'R√©cup√©ration du contexte - Extraction des entit√©s et relations pertinentes du graphe',
          'Prompt enrichi - Le LLM re√ßoit le contexte existant avec le nouveau contenu',
          'D√©duplication intelligente - D√©tection des entit√©s similaires √† celles existantes',
          'Liaison automatique - Cr√©ation de relations avec les n≈ìuds existants'
        ]
      },
      {
        type: 'highlight',
        content: 'Cette approche r√©duit les doublons de 60% et am√©liore la connectivit√© du graphe de 35%.'
      }
    ],
    relatedArticles: ['pipeline-v3', 'entity-resolution']
  },
  {
    id: 'hierarchical-chunking',
    categoryId: 'kg-builder',
    title: 'Hierarchical Chunking',
    subtitle: 'D√©coupage intelligent pr√©servant la structure documentaire',
    icon: 'üìë',
    content: [
      {
        type: 'paragraph',
        content: 'Le Hierarchical Chunking est une technique de d√©coupage qui pr√©serve la structure hi√©rarchique des documents (titres, sections, paragraphes) pour maintenir le contexte lors de l\'extraction.'
      },
      {
        type: 'heading',
        content: 'Avantages par rapport au chunking classique'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Pr√©servation du contexte - Chaque chunk conserve sa position dans la hi√©rarchie',
          'Overlap intelligent - Les chevauchements respectent les limites s√©mantiques',
          'M√©tadonn√©es enrichies - Titre de section, niveau hi√©rarchique, num√©ro de page',
          'Taille adaptative - Les chunks s\'adaptent √† la structure du contenu'
        ]
      },
      {
        type: 'heading',
        content: 'D√©tection de structure'
      },
      {
        type: 'paragraph',
        content: 'L\'algorithme d√©tecte automatiquement les titres via l\'analyse de la taille de police, du style (gras, italique) et des patterns textuels (num√©rotation, mots-cl√©s).'
      }
    ],
    relatedArticles: ['pipeline-v3', 'multi-pass']
  },
  {
    id: 'entity-resolution',
    categoryId: 'kg-builder',
    title: 'Incremental Entity Resolution',
    subtitle: 'R√©solution incr√©mentale des entit√©s pour maintenir la coh√©rence',
    icon: 'üîó',
    content: [
      {
        type: 'paragraph',
        content: 'L\'Entity Resolution incr√©mentale permet de fusionner les mentions d\'une m√™me entit√© √† travers diff√©rents documents tout en maintenant la performance lors de l\'ajout de nouveaux contenus.'
      },
      {
        type: 'heading',
        content: 'Techniques de r√©solution'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Matching exact - Correspondance directe des noms normalis√©s',
          'Matching fuzzy - Similarit√© textuelle avec seuil configurable',
          'Matching s√©mantique - Comparaison des embeddings vectoriels',
          'Matching contextuel - Analyse des relations et attributs communs'
        ]
      },
      {
        type: 'highlight',
        content: 'La r√©solution incr√©mentale permet de traiter des millions d\'entit√©s sans recalculer l\'ensemble du graphe.'
      }
    ],
    relatedArticles: ['graph-aware', 'pipeline-v3']
  },

  // Pipeline Stages Articles
  {
    id: 'stage-parsing',
    categoryId: 'kg-builder',
    title: 'Parsing Stage',
    subtitle: 'Extraction du contenu brut depuis diff√©rents formats de fichiers',
    icon: 'üìÑ',
    content: [
      {
        type: 'paragraph',
        content: 'Le Parsing Stage est la premi√®re √©tape de la pipeline. Il extrait le contenu brut des fichiers sources, qu\'ils soient structur√©s (CSV, JSON) ou non structur√©s (PDF, TXT).'
      },
      {
        type: 'heading',
        content: 'Formats support√©s'
      },
      {
        type: 'list',
        content: '',
        items: [
          'CSV - Auto-d√©tection de l\'encodage et du d√©limiteur',
          'JSON - Parsing des structures imbriqu√©es',
          'PDF - Extraction du texte avec pr√©servation de la mise en page',
          'TXT - Lecture directe du contenu textuel'
        ]
      },
      {
        type: 'heading',
        content: 'Sortie du stage'
      },
      {
        type: 'paragraph',
        content: 'Pour les fichiers structur√©s (CSV/JSON), le parser produit des enregistrements individuels. Pour les documents non structur√©s (PDF/TXT), il produit du texte brut qui sera ensuite d√©coup√© par le Chunking Stage.'
      },
      {
        type: 'highlight',
        content: 'Performance : ~0.05s pour un CSV de 50 lignes, 2-5s pour un PDF de 100 pages.'
      }
    ],
    relatedArticles: ['pipeline-v3', 'stage-chunking', 'stage-extraction']
  },
  {
    id: 'stage-chunking',
    categoryId: 'kg-builder',
    title: 'Chunking Stage',
    subtitle: 'D√©coupage intelligent des documents longs en segments optimis√©s',
    icon: '‚úÇÔ∏è',
    content: [
      {
        type: 'paragraph',
        content: 'Le Chunking Stage d√©coupe les documents longs en segments de taille optimale pour le traitement par les LLMs. Cette √©tape est cruciale pour les PDFs et documents textuels volumineux.'
      },
      {
        type: 'heading',
        content: 'Pourquoi le chunking ?'
      },
      {
        type: 'paragraph',
        content: 'Les LLMs ont une fen√™tre de contexte limit√©e (ex: 8K-128K tokens). Un document de 200 pages contient ~100K+ tokens et doit √™tre divis√© pour √™tre trait√© efficacement.'
      },
      {
        type: 'heading',
        content: 'Configuration'
      },
      {
        type: 'list',
        content: '',
        items: [
          'chunk_size: 1000 tokens (par d√©faut) - Taille cible de chaque segment',
          'chunk_overlap: 200 tokens (par d√©faut) - Chevauchement entre segments',
          'Overlap pr√©serve le contexte aux fronti√®res des chunks'
        ]
      },
      {
        type: 'heading',
        content: 'Quand l\'utiliser ?'
      },
      {
        type: 'paragraph',
        content: 'Utilis√© uniquement pour les documents non structur√©s (PDF, TXT). Les fichiers CSV/JSON sont d√©j√† organis√©s en lignes/enregistrements et n\'ont pas besoin de chunking.'
      },
      {
        type: 'highlight',
        content: 'Astuce : Augmenter chunk_size r√©duit le nombre d\'appels LLM mais peut diminuer la qualit√© du contexte.'
      }
    ],
    relatedArticles: ['hierarchical-chunking', 'stage-parsing', 'stage-embedding']
  },
  {
    id: 'stage-embedding',
    categoryId: 'kg-builder',
    title: 'Embedding Stage',
    subtitle: 'G√©n√©ration de vecteurs s√©mantiques pour la recherche et la r√©solution',
    icon: 'üßÆ',
    content: [
      {
        type: 'paragraph',
        content: 'L\'Embedding Stage g√©n√®re des repr√©sentations vectorielles (embeddings) pour chaque chunk de texte. Ces vecteurs capturent le sens s√©mantique du contenu.'
      },
      {
        type: 'heading',
        content: 'Mod√®le utilis√©'
      },
      {
        type: 'paragraph',
        content: 'Le syst√®me utilise all-MiniLM-L6-v2, un mod√®le compact produisant des vecteurs de 384 dimensions. Il offre un excellent compromis entre qualit√© et performance.'
      },
      {
        type: 'heading',
        content: 'Cas d\'utilisation'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Cross-chunk entity resolution - Identifier la m√™me entit√© dans diff√©rents chunks',
          'Recherche s√©mantique - Trouver des passages similaires par le sens',
          'Extraction contextuelle - Aider le LLM √† comprendre les relations implicites',
          'D√©duplication - D√©tecter les contenus quasi-identiques'
        ]
      },
      {
        type: 'highlight',
        content: 'Les embeddings permettent de comparer "Albert Einstein" et "le physicien allemand" comme s√©mantiquement proches.'
      }
    ],
    relatedArticles: ['stage-chunking', 'stage-ner', 'entity-resolution']
  },
  {
    id: 'stage-ner',
    categoryId: 'kg-builder',
    title: 'NER Stage',
    subtitle: 'Reconnaissance rapide des entit√©s nomm√©es avec spaCy',
    icon: 'üè∑Ô∏è',
    content: [
      {
        type: 'paragraph',
        content: 'Le NER (Named Entity Recognition) Stage effectue une pr√©-extraction rapide des entit√©s nomm√©es avant l\'appel au LLM. Il utilise spaCy pour identifier les candidats potentiels.'
      },
      {
        type: 'heading',
        content: 'Pourquoi avant le LLM ?'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Rapidit√© - spaCy est 100x plus rapide que le LLM',
          'Filtrage - R√©duit la charge de travail du LLM',
          'Indices - Fournit des candidats pour guider l\'extraction LLM'
        ]
      },
      {
        type: 'heading',
        content: 'Types d\'entit√©s d√©tect√©es'
      },
      {
        type: 'paragraph',
        content: 'Le mod√®le spaCy (en_core_web_sm) d√©tecte : personnes, organisations, lieux, dates, montants mon√©taires, pourcentages, et autres entit√©s nomm√©es standard.'
      },
      {
        type: 'highlight',
        content: 'Le NER Stage fonctionne mieux sur des chunks. Il est recommand√© de l\'utiliser apr√®s le Chunking Stage.'
      }
    ],
    relatedArticles: ['stage-embedding', 'stage-extraction', 'pipeline-v3']
  },
  {
    id: 'stage-extraction',
    categoryId: 'kg-builder',
    title: 'Extraction Stage',
    subtitle: 'Extraction des entit√©s et relations via LLM Claude',
    icon: 'ü§ñ',
    content: [
      {
        type: 'paragraph',
        content: 'L\'Extraction Stage est le c≈ìur de la pipeline. Il utilise Claude 3.5 Sonnet via OpenRouter pour extraire les entit√©s et relations √† partir du contenu analys√©.'
      },
      {
        type: 'heading',
        content: 'Modes d\'extraction'
      },
      {
        type: 'list',
        content: '',
        items: [
          'GUIDED - Types d\'entit√©s/relations pr√©d√©finis (Person, Organization, WORKS_AT...)',
          'OPEN - Le LLM d√©couvre librement les types pertinents',
          'HYBRID - Combine les deux : types guid√©s + d√©couverte libre'
        ]
      },
      {
        type: 'heading',
        content: 'Fonctionnalit√©s'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Traitement par batch (50 enregistrements/batch)',
          'D√©duplication automatique des entit√©s similaires',
          'Pattern Strategy pour changer de mode facilement',
          'Gestion des erreurs et retry automatique'
        ]
      },
      {
        type: 'highlight',
        content: 'L\'extraction est le goulot d\'√©tranglement de la pipeline (~60-70s pour 50 lignes CSV, ~120-180s pour un PDF de 100 pages).'
      }
    ],
    relatedArticles: ['multi-pass', 'graph-aware', 'stage-validation']
  },
  {
    id: 'stage-validation',
    categoryId: 'kg-builder',
    title: 'Validation Stage',
    subtitle: 'V√©rification de la qualit√© et coh√©rence des donn√©es extraites',
    icon: '‚úÖ',
    content: [
      {
        type: 'paragraph',
        content: 'Le Validation Stage v√©rifie la qualit√© et la coh√©rence des entit√©s et relations extraites avant leur stockage dans Neo4j.'
      },
      {
        type: 'heading',
        content: 'V√©rifications effectu√©es'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Champs requis - V√©rifie que name, type sont pr√©sents',
          'R√©f√©rences d\'entit√©s - V√©rifie que les relations pointent vers des entit√©s existantes',
          'Auto-r√©f√©rences - D√©tecte les relations d\'une entit√© vers elle-m√™me',
          'Types valides - V√©rifie la conformit√© aux types configur√©s (mode GUIDED)'
        ]
      },
      {
        type: 'heading',
        content: 'Modes de validation'
      },
      {
        type: 'paragraph',
        content: 'Mode strict : rejette les donn√©es invalides. Mode lenient : accepte avec warnings. Configurable selon le cas d\'usage.'
      },
      {
        type: 'highlight',
        content: 'Performance ultra-rapide : ~0.01s pour un CSV, ~0.05s pour un PDF. Ne bloque jamais la pipeline.'
      }
    ],
    relatedArticles: ['stage-extraction', 'stage-storage', 'pipeline-v3']
  },
  {
    id: 'stage-storage',
    categoryId: 'kg-builder',
    title: 'Storage Stage',
    subtitle: 'Persistance des donn√©es dans Neo4j avec indexation optimis√©e',
    icon: 'üíæ',
    content: [
      {
        type: 'paragraph',
        content: 'Le Storage Stage persiste les entit√©s et relations valid√©es dans la base Neo4j. Il g√®re √©galement la g√©n√©ration des embeddings pour la recherche vectorielle.'
      },
      {
        type: 'heading',
        content: 'Caract√©ristiques'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Op√©rations batch (50 √©l√©ments/batch) pour performance optimale',
          'MERGE Cypher pour idempotence (pas de doublons)',
          'G√©n√©ration automatique des embeddings (all-MiniLM-L6-v2, 384 dims)',
          'Indexation pour recherche rapide'
        ]
      },
      {
        type: 'heading',
        content: 'Embeddings au stockage'
      },
      {
        type: 'paragraph',
        content: 'Pour les fichiers CSV/JSON qui n\'ont pas pass√© par l\'Embedding Stage, les vecteurs sont g√©n√©r√©s automatiquement lors du stockage. Cela permet la recherche s√©mantique sur toutes les entit√©s.'
      },
      {
        type: 'highlight',
        content: 'Le Storage Stage retourne les IDs Neo4j cr√©√©s, permettant de tracer chaque entit√© jusqu\'√† son document source.'
      }
    ],
    relatedArticles: ['stage-validation', 'pipeline-v3', 'graph-aware']
  },

  // Graph RAG Articles
  {
    id: 'graph-rag-overview',
    categoryId: 'jarvis-assistant',
    title: 'Graph RAG Overview',
    subtitle: 'Retrieval-Augmented Generation enrichi par Knowledge Graph',
    icon: 'üéØ',
    content: [
      {
        type: 'paragraph',
        content: 'Le Graph RAG (Retrieval-Augmented Generation) est l\'approche centrale de Jarvis pour r√©pondre aux questions. Au lieu de se fier uniquement aux connaissances du LLM, le syst√®me r√©cup√®re des informations pertinentes depuis le Knowledge Graph pour contextualiser ses r√©ponses.'
      },
      {
        type: 'heading',
        content: 'Avantages du Graph RAG'
      },
      {
        type: 'list',
        content: '',
        items: [
          'R√©ponses factuelles - Bas√©es sur vos donn√©es, pas sur les connaissances g√©n√©riques du LLM',
          'R√©duction des hallucinations - Le contexte KG ancre les r√©ponses dans la r√©alit√©',
          'Tra√ßabilit√© - Chaque information peut √™tre rattach√©e √† sa source',
          'Personnalisation - Le syst√®me apprend et utilise vos connaissances sp√©cifiques'
        ]
      },
      {
        type: 'heading',
        content: 'Architecture du flux'
      },
      {
        type: 'paragraph',
        content: 'Le flux Graph RAG suit 6 √©tapes principales : Classification d\'intent ‚Üí Extraction NER ‚Üí Recherche s√©mantique ‚Üí Ranking multi-facteurs ‚Üí Construction du contexte ‚Üí Appel LLM. Chaque √©tape affine les r√©sultats pour maximiser la pertinence.'
      },
      {
        type: 'highlight',
        content: 'Le Graph RAG r√©duit les hallucinations de 70% sur les questions factuelles par rapport √† un LLM sans contexte.'
      }
    ],
    relatedArticles: ['semantic-retrieval', 'embeddings-role', 'intelligent-routing']
  },
  {
    id: 'semantic-retrieval',
    categoryId: 'jarvis-assistant',
    title: 'Semantic Retrieval',
    subtitle: 'Recherche vectorielle dans le Knowledge Graph',
    icon: 'üîç',
    content: [
      {
        type: 'paragraph',
        content: 'Le Semantic Retrieval est le processus de recherche des entit√©s pertinentes dans le Knowledge Graph. Il combine la recherche vectorielle (embeddings) avec la travers√©e du graphe pour trouver le contexte optimal.'
      },
      {
        type: 'heading',
        content: 'Processus de recherche'
      },
      {
        type: 'list',
        content: '',
        items: [
          '1. Embedding de la requ√™te - La question utilisateur est convertie en vecteur 384D',
          '2. Recherche par similarit√© - Cosine similarity avec tous les embeddings du graphe',
          '3. Filtrage par seuil - Seuls les r√©sultats avec score > 0.15 sont conserv√©s',
          '4. R√©cup√©ration des relations - Chaque entit√© trouv√©e inclut ses 10 premi√®res relations'
        ]
      },
      {
        type: 'heading',
        content: 'Mod√®le d\'embedding'
      },
      {
        type: 'paragraph',
        content: 'Le syst√®me utilise all-MiniLM-L6-v2, un mod√®le de Sentence Transformers produisant des vecteurs de 384 dimensions. Ce mod√®le offre un excellent compromis entre qualit√© s√©mantique et rapidit√© d\'inf√©rence.'
      },
      {
        type: 'highlight',
        content: 'La recherche retourne les top 20 candidats avec leurs scores de similarit√© et relations associ√©es.'
      }
    ],
    relatedArticles: ['graph-rag-overview', 'embeddings-role', 'multi-factor-ranking']
  },
  {
    id: 'embeddings-role',
    categoryId: 'jarvis-assistant',
    title: 'Role des Embeddings',
    subtitle: 'Comment les vecteurs s√©mantiques alimentent le RAG',
    icon: 'üßÆ',
    content: [
      {
        type: 'paragraph',
        content: 'Les embeddings sont des repr√©sentations vectorielles qui capturent le sens s√©mantique du texte. Dans Jarvis, chaque entit√© du Knowledge Graph poss√®de un embedding stock√© directement dans Neo4j.'
      },
      {
        type: 'heading',
        content: 'G√©n√©ration des embeddings'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Texte source = nom de l\'entit√© + propri√©t√©s cl√©s',
          'Mod√®le: all-MiniLM-L6-v2 (Sentence Transformers)',
          'Dimension: 384 valeurs float',
          'Stockage: Propri√©t√© .embedding sur chaque n≈ìud Neo4j'
        ]
      },
      {
        type: 'heading',
        content: 'Usages dans le pipeline'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Semantic Search - Trouver les entit√©s similaires √† la question',
          'Entity Resolution - Identifier "Einstein" et "Albert Einstein" comme identiques',
          'KG Awareness - √âvaluer si le KG contient des infos pertinentes',
          'Cross-document linking - Relier des entit√©s de diff√©rents documents'
        ]
      },
      {
        type: 'highlight',
        content: 'Les embeddings permettent de comprendre que "le physicien qui a d√©couvert la relativit√©" correspond √† "Albert Einstein" m√™me sans correspondance textuelle exacte.'
      }
    ],
    relatedArticles: ['semantic-retrieval', 'stage-embedding', 'entity-resolution']
  },
  {
    id: 'multi-factor-ranking',
    categoryId: 'jarvis-assistant',
    title: 'Multi-Factor Ranking',
    subtitle: 'Scoring combin√© Embedding + NER + Graph Centrality',
    icon: 'üìä',
    content: [
      {
        type: 'paragraph',
        content: 'Le ranking multi-facteurs combine trois signaux diff√©rents pour s√©lectionner les entit√©s les plus pertinentes. Cette approche d√©passe la simple similarit√© vectorielle en int√©grant la structure du graphe.'
      },
      {
        type: 'heading',
        content: 'Formule de scoring'
      },
      {
        type: 'code',
        content: 'final_score = (similarity √ó 0.5) + (type_match √ó 0.2) + (centrality √ó 0.3)',
        language: 'text'
      },
      {
        type: 'heading',
        content: 'Les trois facteurs'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Similarity (50%) - Score cosine entre embedding requ√™te et entit√©',
          'Type Match (20%) - Correspondance entre types NER extraits et types d\'entit√©s',
          'Graph Centrality (30%) - Nombre de relations normalis√© (max 10), favorise les entit√©s connect√©es'
        ]
      },
      {
        type: 'heading',
        content: 'Exemple concret'
      },
      {
        type: 'paragraph',
        content: 'Pour "Qui est le directeur de TechCorp?", une entit√© Person avec similarit√© 0.7, type PERSON match, et 8 relations obtient: (0.7√ó0.5) + (1.0√ó0.2) + (0.8√ó0.3) = 0.79. Elle sera class√©e avant une entit√© avec meilleure similarit√© mais moins connect√©e.'
      },
      {
        type: 'highlight',
        content: 'Le ranking s√©lectionne les top 5 candidats qui seront format√©s dans le contexte envoy√© au LLM.'
      }
    ],
    relatedArticles: ['semantic-retrieval', 'graph-rag-overview', 'intelligent-routing']
  },
  {
    id: 'intelligent-routing',
    categoryId: 'jarvis-assistant',
    title: 'Intelligent Routing',
    subtitle: 'Routage dynamique bas√© sur la pertinence du KG',
    icon: 'üß≠',
    content: [
      {
        type: 'paragraph',
        content: 'L\'Intelligent Routing (Sprint 13) est le cerveau d√©cisionnel de l\'orchestrateur. Il d√©termine dynamiquement si et comment utiliser le Knowledge Graph pour r√©pondre √† une question.'
      },
      {
        type: 'heading',
        content: 'KG Awareness'
      },
      {
        type: 'paragraph',
        content: 'Avant de d√©cider du routing, le syst√®me sonde le KG pour calculer un kg_match_score (0.0-1.0). Ce score combine la meilleure similarit√© (60%) et la moyenne des top r√©sultats (40%).'
      },
      {
        type: 'heading',
        content: 'R√®gles de routage'
      },
      {
        type: 'list',
        content: '',
        items: [
          'full_kg - Score ‚â• 0.5 ‚Üí Pipeline KG complet (NER + Retrieval + Ranking)',
          'no_match - Score < 0.2 ‚Üí Skip KG, r√©ponse LLM directe',
          'direct - Intent salutation/heure ‚Üí R√©ponse imm√©diate sans KG ni LLM'
        ]
      },
      {
        type: 'heading',
        content: 'Query Decomposition'
      },
      {
        type: 'paragraph',
        content: 'Pour les requ√™tes complexes ("Qui est Einstein et quelles sont ses d√©couvertes?"), le syst√®me d√©compose en sous-t√¢ches via LLM, puis agr√®ge les r√©sultats.'
      },
      {
        type: 'highlight',
        content: 'Le routing intelligent √©vite les requ√™tes KG inutiles et acc√©l√®re les r√©ponses g√©n√©rales de 40%.'
      }
    ],
    relatedArticles: ['graph-rag-overview', 'multi-factor-ranking', 'agent-orchestration']
  },
  {
    id: 'kg-gds-combination',
    categoryId: 'jarvis-assistant',
    title: 'KG + Embeddings + GDS',
    subtitle: 'L\'approche hybride Knowledge Graph + Graph Data Science',
    icon: 'üî¨',
    content: [
      {
        type: 'paragraph',
        content: 'Jarvis combine trois technologies compl√©mentaires : le Knowledge Graph (structure), les Embeddings (s√©mantique) et le Graph Data Science (analyse). Cette trinit√© offre une compr√©hension riche des donn√©es.'
      },
      {
        type: 'heading',
        content: 'Knowledge Graph - La Structure'
      },
      {
        type: 'paragraph',
        content: 'Neo4j stocke les entit√©s (n≈ìuds) et leurs relations (ar√™tes). Cette structure permet de naviguer les connexions : "Marie TRAVAILLE_√Ä TechCorp", "TechCorp BAS√â_√Ä Paris". Le graphe capture les relations explicites entre concepts.'
      },
      {
        type: 'heading',
        content: 'Embeddings - La S√©mantique'
      },
      {
        type: 'paragraph',
        content: 'Les vecteurs de 384 dimensions capturent le sens au-del√† des mots. "Entreprise" et "Soci√©t√©" ont des embeddings proches m√™me sans lien textuel. Cela permet la recherche par similarit√© de sens.'
      },
      {
        type: 'heading',
        content: 'Graph Data Science - L\'Analyse'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Centralit√© - Identifier les entit√©s les plus connect√©es/importantes',
          'Communaut√©s - D√©tecter les clusters d\'entit√©s li√©es',
          'Similarit√© structurelle - Trouver des entit√©s avec patterns de relations similaires',
          'Chemins - Calculer les connexions entre deux entit√©s'
        ]
      },
      {
        type: 'highlight',
        content: 'La combinaison KG+Embeddings+GDS permet de r√©pondre √† des questions que chaque composant seul ne pourrait pas traiter efficacement.'
      }
    ],
    relatedArticles: ['multi-factor-ranking', 'embeddings-role', 'graph-rag-overview']
  },

  // Jarvis Assistant Articles
  {
    id: 'voice-interface',
    categoryId: 'jarvis-assistant',
    title: 'Voice Interface',
    subtitle: 'Interface vocale push-to-talk avec transcription et synth√®se',
    icon: 'üéôÔ∏è',
    content: [
      {
        type: 'paragraph',
        content: 'L\'interface vocale de Jarvis permet une interaction naturelle via la parole. Elle combine transcription automatique (STT) et synth√®se vocale (TTS) pour une exp√©rience conversationnelle fluide.'
      },
      {
        type: 'heading',
        content: 'Technologies utilis√©es'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Whisper (OpenAI) - Transcription haute qualit√© multilingue',
          'Groq - Alternative rapide pour la transcription temps r√©el',
          'Edge TTS - Synth√®se vocale naturelle de Microsoft',
          'Coqui TTS - Option open-source pour la synth√®se'
        ]
      },
      {
        type: 'heading',
        content: 'Mode Push-to-Talk'
      },
      {
        type: 'paragraph',
        content: 'Le mode push-to-talk permet un contr√¥le pr√©cis de l\'enregistrement. Maintenez le bouton pour parler, rel√¢chez pour envoyer. Des indicateurs visuels montrent l\'√©tat de l\'enregistrement et du traitement.'
      }
    ],
    relatedArticles: ['agent-orchestration', 'multi-llm']
  },
  {
    id: 'agent-orchestration',
    categoryId: 'jarvis-assistant',
    title: 'Agent Orchestration',
    subtitle: 'Orchestrateur central routant vers des agents sp√©cialis√©s',
    icon: 'üéØ',
    content: [
      {
        type: 'paragraph',
        content: 'L\'orchestrateur Jarvis est le cerveau du syst√®me. Il analyse les requ√™tes utilisateur et les route vers les agents sp√©cialis√©s les plus appropri√©s pour y r√©pondre.'
      },
      {
        type: 'heading',
        content: 'Agents disponibles'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Agent KG - Interrogation et exploration du Knowledge Graph',
          'Agent Search - Recherche d\'informations externes',
          'Agent Analysis - Analyse de donn√©es et g√©n√©ration de rapports',
          'Agent General - Conversations g√©n√©rales et questions diverses'
        ]
      },
      {
        type: 'heading',
        content: 'Routage intelligent'
      },
      {
        type: 'paragraph',
        content: 'Le routage utilise une combinaison de classification par mots-cl√©s et d\'analyse s√©mantique pour d√©terminer l\'agent optimal. Le contexte de la conversation est √©galement pris en compte.'
      }
    ],
    relatedArticles: ['kg-augmented', 'voice-interface']
  },
  {
    id: 'kg-augmented',
    categoryId: 'jarvis-assistant',
    title: 'KG-Augmented Responses',
    subtitle: 'R√©ponses enrichies par le Knowledge Graph',
    icon: 'üìä',
    content: [
      {
        type: 'paragraph',
        content: 'Les r√©ponses KG-Augmented utilisent le Knowledge Graph comme source de v√©rit√© pour enrichir les r√©ponses du LLM avec des informations factuelles et contextuelles pr√©cises.'
      },
      {
        type: 'heading',
        content: 'Processus d\'augmentation'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Extraction des entit√©s - Identification des entit√©s mentionn√©es dans la question',
          'Requ√™te du graphe - R√©cup√©ration des informations pertinentes depuis Neo4j',
          'Contextualisation - Int√©gration des donn√©es du graphe dans le prompt',
          'G√©n√©ration - Le LLM produit une r√©ponse inform√©e par le contexte'
        ]
      },
      {
        type: 'highlight',
        content: 'Cette approche r√©duit les hallucinations du LLM de 70% pour les questions factuelles.'
      }
    ],
    relatedArticles: ['agent-orchestration', 'pipeline-v3']
  },
  {
    id: 'multi-llm',
    categoryId: 'jarvis-assistant',
    title: 'Multi-LLM Support',
    subtitle: 'Support de plusieurs mod√®les LLM via OpenRouter',
    icon: 'üß©',
    content: [
      {
        type: 'paragraph',
        content: 'Jarvis supporte plusieurs mod√®les de langage gr√¢ce √† l\'int√©gration OpenRouter. Cela permet de choisir le mod√®le optimal selon le cas d\'usage et le budget.'
      },
      {
        type: 'heading',
        content: 'Mod√®les support√©s'
      },
      {
        type: 'list',
        content: '',
        items: [
          'Claude (Anthropic) - Excellent pour le raisonnement et l\'analyse',
          'GPT-4 (OpenAI) - Polyvalent et performant',
          'Mistral - Rapide et √©conomique',
          'Llama - Option open-source'
        ]
      },
      {
        type: 'heading',
        content: 'Configuration'
      },
      {
        type: 'paragraph',
        content: 'Le mod√®le peut √™tre configur√© dans les param√®tres. Il est possible de d√©finir des mod√®les diff√©rents pour l\'orchestrateur et les agents sp√©cialis√©s.'
      }
    ],
    relatedArticles: ['agent-orchestration', 'voice-interface']
  }
]

export function getArticleById(id: string): Article | undefined {
  return articles.find(a => a.id === id)
}

export function getArticlesByCategory(categoryId: string): Article[] {
  return articles.filter(a => a.categoryId === categoryId)
}

export function getRelatedArticles(articleId: string): Article[] {
  const article = getArticleById(articleId)
  if (!article?.relatedArticles) return []
  return article.relatedArticles
    .map(id => getArticleById(id))
    .filter((a): a is Article => a !== undefined)
}
